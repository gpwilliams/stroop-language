---
title: "Results"
author: "Dr Glenn Williams"
format:
  html:
    code-fold: true
    page-layout: full
editor: source
toc: true
number-sections: false
execute:
  warning: false
  message: false
bibliography: citations.bib
csl: apa.csl
---

```{r setup}
#| message: false

# load packages ----

library(tidyverse)
library(here)
library(gt)
library(english)
library(glue)

# set options ----

options(scipen = 999)

# functions ----

list.files(here("R", "00_load-functions"), full.names = TRUE) |> 
  purrr::walk(source)

# process data ----

# data summaries
performance_exclusions <- read_csv(here(
  "01_data", 
  "01_raw", 
  "performance_exclusions.csv"
))
data_checks <- map_files_to_list(
  here("04_analysis", "01_data-checks"), 
  file_type = ".csv"
)
descriptives <- map_files_to_list(
  here("04_analysis", "02_descriptives"), 
  file_type = ".csv"
)

# models and model products
freq_results <- map_files_to_list(
  here("04_analysis", "04_model-predictions", "01_frequentist"),
  file_type = ".csv"
)

# make df into a single column for ANOVA tables
freq_results[c(
  "accuracy_anova_freq_table", 
  "rt_anova_freq_table",
  "rt_mixed_freq_table_anova"
)] <- freq_results[c(
  "accuracy_anova_freq_table", 
  "rt_anova_freq_table",
  "rt_mixed_freq_table_anova"
)] |>
  map( 
    ~ .x |> 
      mutate(df = paste0("[", num_df, ", ", round(den_df), "]")) |> 
      select(-c(num_df, den_df))
  )

# make df label consistent across all tables
freq_results["accuracy_mixed_freq_table_anova"] <- 
  freq_results["accuracy_mixed_freq_table_anova"] |>
  map( 
    ~ .x |> 
      mutate(df = as.character(chi_df)) |> 
      select(-chi_df)
  )

# make all parameter names consistent
# fix printing of p-values for APA formatting
freq_results <- freq_results |> 
  map( 
    ~ .x |> 
      mutate(p_value = papa(p_value, asterisk = FALSE)) |> 
      mutate_if(is.numeric, round_pad) |> 
      mutate( # fix inconsistent ordering
       Parameter = case_when(
          Parameter == "(Intercept)" ~ "Intercept",
          Parameter == "Trial Type × Language" ~ "Language × Trial Type",
          Parameter == "Stroop × Trial Type × Language" ~ "Stroop × Language × Trial Type",
          TRUE ~ Parameter
        )
      ) |> 
      arrange( # sort to specific order across data sets
        Study,
        Measure,
        factor(
          Parameter,
          levels = c(
            "Intercept",
            "Stroop",
            "Language",
            "Trial Type",
            "Stroop × Language",
            "Stroop × Trial Type",
            "Language × Trial Type",
            "Stroop × Language × Trial Type"
          ))
      )
  )

# load bayesian data
bayes_results <- map_files_to_list(
  here("04_analysis", "04_model-predictions", "02_bayesian"),
  file_type = ".csv"
)

# compute bayes factors in support of the null hypothesis
# find when bayes factors are inconsistent across models 3-5
# due to prior sensitivity
# fix printing of Bayes factors to limit max values
# fix printing of parameter names for consistency
bayes_results[grep('bayes-factors', names(bayes_results))] <- 
  bayes_results[grep('bayes-factors', names(bayes_results))] |> 
  map(
    ~ .x |> 
      mutate(BF = 1/BF) |> # make BF into evidence for null
      rename(BF_01 = BF) |> # rename to make this clear
      select(Study, Prior_Model, Parameter, BF_01) |> 
      pivot_wider(names_from = Prior_Model, values_from = BF_01) |> 
      mutate(
        prior_sensitive = case_when(
          `3` <= 1/3 & `4` <= 1/3 & `5` <= 1/3 ~ FALSE, # all negative
          `3` >= 3 & `4` >= 3 & `5` >= 3 ~ FALSE, # all positive
          `3` <= 3 & `3` >= 1/3 & `4` <= 3 & `3` >= 
            1/3 & `4` <= 3 & `5` >= 1/3 ~ FALSE, # all insensitive
        TRUE ~ TRUE
        )
      ) |> 
      rename(BF_01 = `4`) |>  # keep model 4
      select(Study, Parameter, BF_01, prior_sensitive) |> 
      mutate(
        BF_01 = case_when(
          BF_01 > 1000 ~ "> 1000",
          BF_01 < 1/1000 ~ "< 0.001",
          TRUE ~ round_pad(BF_01, digits = 3, nsmall = 2)
        ),
        Parameter = case_when(
          Parameter == "Trial Type × Language" ~ "Language × Trial Type",
          Parameter == "Stroop × Trial Type × Language" ~ "Stroop × Language × Trial Type",
          TRUE ~ Parameter
        )
      ) |> 
      mutate_if(is.numeric, round_pad) 
  )

# fix printing for all values
bayes_results <- bayes_results |> 
    map( ~ .x |>  mutate_if(is.numeric, round_pad))

# get pairwise tests, fix p-values for printing, round numbers
pairwise_tests <- map_files_to_list(
  here("04_analysis", "05_pairwise-tests"),
  file_type = ".csv"
) 

pairwise_tests_plots <- pairwise_tests

# fix printing of pairwise tests
pairwise_tests[grep('pairs', names(pairwise_tests))] <- 
  pairwise_tests[grep('pairs', names(pairwise_tests))] |> 
    map( 
    ~ .x |> 
      rename(p_value = p.value) |> 
      mutate(p_value = papa(p_value, asterisk = FALSE))
  )

pairwise_tests <- pairwise_tests |> 
  map( ~ .x |> mutate_if(is.numeric, round_pad))
```

# Experiment 1

The hypothesis, sample size, and analyses of all experiments presented here were pre-registered (Experiment 1: [https://osf.io/ruc4x/](https://osf.io/ruc4x/); Experiment 2: [https://osf.io/emz5v/](https://osf.io/emz5v/); Experiment 3: [https://osf.io/q9dwc/](https://osf.io/q9dwc/)). The experiments received approval by Abertay University’s ethics committee (EMS3231).

## Participants

`r report_participant_numbers(data_checks$participant_numbers, "dutch_stroop")`[^1] Dutch-English bilinguals were recruited. `r report_recording_failure(performance_exclusions, "dutch_stroop")` participant was excluded due to a recording failure and `r report_under_age(data_checks$additional_exclusions, "dutch_stroop")` participant was removed due to being under-age, leaving `r report_participant_counts(descriptives$gender_counts, "dutch_stroop")` participants with a mean age of `r report_participant_age(descriptives$demo_summary, "dutch_stroop")`. Following the experiment, participants were asked to complete a language background questionnaire [cf. @kirk2018can], and English and Dutch vocabulary tests based on lexical decision tasks [i.e., LexTale, @lemhofer2012introducing, see @tbl-dutch-demo].

[^1]: We pre-registered 40 participants as our target sample size. However, more participants took part than expected in a short time frame before we could stop data collection. As this was not a conscious decision to increase the sample size (i.e. optional stopping), we include these additional participants in our analysis because this should not increase type-I error rates and will increase accuracy of parameter estimates and inferences.

```{r tbl-dutch-demo}
#| label: tbl-dutch-demo
#| tbl-cap: Mean percentages (SD) for language use and proficiency for each language for participants in Experiment 1
descriptives$demo_summary |> 
  filter(
    study == "dutch_stroop", 
    str_detect(parameter, "current_percent|lex_tale"), 
    statistic %in% c("mean", "sd")
  ) |> 
  pivot_wider(names_from = statistic, values_from = value) |> 
  mutate_if(is.numeric, round_pad) |> 
  mutate(
    Parameter = case_when(
      parameter == "current_percent_english" ~ "English Current Use",
      parameter == "current_percent_english_pair" ~ "Dutch Current Use",
      parameter == "lex_tale_english" ~ "English LexTale",
      parameter == "lex_tale_english_pair" ~ "Dutch LexTale"
    ),
    Score = paste0(mean, " (", sd, ")")
  ) |> 
  select(Parameter, Score) |> 
  apa_gt()
```

## Outliers

Error trials and trials where no or another extraneous sound were detected were excluded from RT analyses, as were “recovery” trials following an error trial. For accuracy and reaction time analyses, the first trial in each block and "recovery" trials following an error were excluded from the analyses. For reaction time analyses only, reaction times under 150ms, or reaction times over 2500ms or more than three standard deviations above the participant mean were discarded as outliers. Taking these criteria into account, a total of `r report_dropped_trial_numbers_accuracy(data_checks$trial_numbers, "dutch_stroop")`% and `r report_dropped_trial_numbers_rt(data_checks$trial_numbers, "dutch_stroop")`% of trials were excluded from the accuracy and reaction time analyses respectively. 

## Analysis

We used R [@R-base] and the R-packages tidyverse [@tidyverse], here [@here], and bayesplot [@bayesplot-a; @bayesplot-b] for data preparation, analysis, and presentation. For each study we performed separate frequentist and Bayesian[^2] analyses for both the accuracy and response time dependent variables. For the frequentist analyses, we modelled the data with multilevel models fitted using the lme4 R-package [@lme4]. We used the afex R-package [@afex] to fit models with nested fixed effects, evaluating the statistical significance of main effects and interactions using likelihood-ratio tests. For the parameter estimates of each model, statistical significance is evaluated using *p*-values approximated using the Satterthwaite method implemented in the lmerTest R-package [@lmerTest]. Significant interactions were further investigated using estimated marginal means and pairwise tests calculated using the emmeans R-package [@emmeans]. We fitted Bayesian equivalents to the frequentist multilevel models using the brms R-package [@brms-a; @brms-b]. Bayes Factors were calculated using the Savage-Dickey approximation using the bayestestR R-package [@bayestestR].

[^2]: Bayesian analyses are included primarily to interpret the interaction between Language, Stroop Block, and Trial Type as per our pre-registered analyses for Experiments 2 and 3. For completeness, we report Bayes factors for all parameters and for Experiment 1.

Given the lower bound and skewed nature of response times, this outcome was modelled using linear mixed effects models assuming a log-normal distribution[^3]. Given accuracy scores are binary, this outcome was modelled using generalised linear mixed effects models assuming a binomial distribution with a logit link function. All models used sum-coded (-1, 1) fixed effects of Stroop Block (Neutral or Incongruent), Trial Type (Repetition or Switch), and Language (English or Dutch). Models contained crossed random effects of participants and items, using the maximal random effects structure justified by the design that allows for model convergence [@barr2013random]. One advantage of Bayesian analyses is that with relatively informative priors the maximal random effects structure can be fitted without convergence problems [@vasishth2016statistical; @nicenboim2016statistical], meaning that no simplification to random effects was needed. For a description of the random effects in each frequentist model, see Appendix A. For a description of priors in each Bayesian model and how these affect parameter estimates and Bayes factors, see Appendix B. Unless otherwise stated, Bayes factors are consistent across different prior specifications from the prior sensitivity analysis. Finally, while the mixed effects models form the basis of our conclusions, for direct comparison to Liu et al. [-@liu2019symmetries], we also analysed the data aggregated by participant using ANOVAs evaluating the effect of Stroop Block and Language directly on switch costs (i.e. Switch trials - Non-Switch trials). 

[^3]: For frequentist models fitted in lme4 this requires first transforming response times to their natural log before fitting. For Bayesian models fitted in brms response times are fitted directly using a lognormal family.

For Experiment 1, response times and the accuracies are shown in each Language split by Stroop Block and Trial Type in @fig-dutch-plot.

```{r fig-dutch-plot}
#| label: fig-dutch-plot
#| fig-cap: "Response times and proportions of correct responses by Language, Stroop Block, and Trial Type in Experiment 1"
#| fig-alt: "A raincloud plot showing the response times and proportion of correct responses in English and Dutch."
knitr::include_graphics(here("03_plots", "03_descriptive", "dutch_patchwork.png"))
```

### Response Time

Fixed effects parameter estimates for response time models are shown in @tbl-param-dutch-rt.

```{r tbl-param-dutch-rt}
#| label: tbl-param-dutch-rt
#| tbl-cap: Fixed effects parameter estimates and test statistics for response time in Experiment 1
bind_parameters_rt(
  freq_results$rt_mixed_freq_table_est,
  bayes_results$`rt_bayes-factors`,
  "Dutch"
) |> 
  parameters_rt_gt()
```

```{r dutch-rt-emms}
#| include: false
dutch_stroop_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "dutch", analysis == "stroop")

dutch_trial_type_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "dutch", analysis == "trial-type")
```

There was a significant main effect of Stroop Block with faster response times in the Neutral condition `r make_descriptives_sentence(dutch_stroop_rt_emm, "Neutral", "emmean", "lower.CL", "upper.CL")` than the Incongruent condition `r make_descriptives_sentence(dutch_stroop_rt_emm, "Incongruent", "emmean", "lower.CL", "upper.CL")`. There was also a significant main effect of Trial Type, with faster response times for Repetition `r make_descriptives_sentence(dutch_trial_type_rt_emm, "Repetition", "emmean", "lower.CL", "upper.CL")` than for Switch trials `r make_descriptives_sentence(dutch_trial_type_rt_emm, "Switch", "emmean", "lower.CL", "upper.CL")`. All other effects were non-significant, with Bayes factors \> 3, indicating the data are at least 3 times more likely under the null model than the alternative model. 

The ANOVA showed a main effect of Stroop Block `r make_anova_sentence(freq_results$rt_anova_freq_table, "Dutch", "Stroop")` and Language `r make_anova_sentence(freq_results$rt_anova_freq_table, "Dutch", "Language")`, but no interaction between Stroop Block and Language (*p* > .05). 

### Accuracy

Fixed effects parameter estimates for accuracy models are shown in @tbl-param-dutch-accuracy.

```{r tbl-param-dutch-accuracy}
#| label: tbl-param-dutch-accuracy
#| tbl-cap: Fixed effects parameter estimates and test statistics for accuracy in Experiment 1
bind_parameters_accuracy(
  freq_results$accuracy_mixed_freq_table_est,
  bayes_results$`accuracy_bayes-factors`,
  "Dutch"
) |>
  parameters_accuracy_gt()
```

```{r dutch-accuracy-emm}
#| include: false
dutch_stroop_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "dutch", analysis == "stroop")

dutch_trial_type_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "dutch", analysis == "trial-type")
```

There was a significant main effect of Stroop Block with a larger proportion of correct answers in the Neutral condition `r make_descriptives_sentence(dutch_stroop_accuracy_emm, "Neutral", "prob", "asymp.LCL", "asymp.UCL")` than the Incongruent condition `r make_descriptives_sentence(dutch_stroop_accuracy_emm, "Incongruent", "prob", "asymp.LCL", "asymp.UCL")`. There was also a significant main effect of Trial Type, with a larger proportion of correct answers for Repetition `r make_descriptives_sentence(dutch_trial_type_accuracy_emm, "Repetition", "prob", "asymp.LCL", "asymp.UCL")` than for Switch trials `r make_descriptives_sentence(dutch_trial_type_accuracy_emm, "Switch", "prob", "asymp.LCL", "asymp.UCL")`. All other effects were non-significant. Of these, only the Language × Trial Type interaction provided reliable evidence in support of the null hypothesis that was insensitive to prior specification. 

The ANOVA showed a significant main effect of Stroop Block `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Dutch", "Language")` and Language `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Dutch", "Language")`, but no significant interaction between Stroop Block and Language (*p* > .05). 

# Experiment 2

Focusing primarily on the findings related to our primary hypotheses, we found no reliable evidence for a three-way interaction between Language, Stroop Block, and Trial Type. In fact, for response times we found evidence against these interactions. In aggregated analyses most similar to that used by @liu2019symmetries we again found no evidence of an interaction between Language and Stroop block for switch costs for response times or accuracy. 

One possibly important difference between Experiment 1 of the present study and that of @liu2019symmetries is that the latter tested bilinguals with different scripts (Chinese-English), whereas the former used languages in the same script (Dutch-English). Since previous research has shown that languages in different scripts can affect language switching relative to when the scripts are similar [@slevc2016new] and because the main manipulation (i.e., the Stroop effect) relies on reading, relying on different script bilinguals could have resulted in a different pattern in @liu2019symmetries than what we observed in Experiment 1. To accommodate for this difference across studies, Experiment 2 used the same methodology as in Experiment 1 of the current study, but tested bilinguals with different scripts across their languages (Arabic-English).

## Participants

`r report_participant_numbers(data_checks$participant_numbers, "arabic_stroop")` Arabic-English bilinguals were recruited. `r report_recording_failure(performance_exclusions, "arabic_stroop")` participants were excluded due to a recording failure and `r report_performance_exclusions(performance_exclusions, "arabic_stroop")` participants were excluded due to non-compliance with instructions, leaving `r report_participant_counts(descriptives$gender_counts, "arabic_stroop")` participants with a mean age of `r report_participant_age(descriptives$demo_summary, "arabic_stroop")`. Following the experiment, participants were asked to complete a the same language background questionnaire and English and Arabic vocabulary tests based on lexical decision tasks as in Experiment 1. Following the experiment, the participants were asked to complete a language background questionnaire [cf. @kirk2018can], and English and Arabic vocabulary tests based on lexical decision tasks [@hamed2018role; @lemhofer2012introducing\; see @tbl-arabic-demo].

```{r tbl-arabic-demo}
#| label: tbl-arabic-demo
#| tbl-cap: Mean percentages (SD) for language use and proficiency for each language for participants in Experiment 2
descriptives$demo_summary |> 
  filter(
    study == "arabic_stroop", 
    str_detect(parameter, "current_percent|lex_tale"), 
    statistic %in% c("mean", "sd")
  ) |> 
  pivot_wider(names_from = statistic, values_from = value) |> 
  mutate_if(is.numeric, round_pad) |> 
  mutate(
    Parameter = case_when(
      parameter == "current_percent_english" ~ "English Current Use",
      parameter == "current_percent_english_pair" ~ "Arabic Current Use",
      parameter == "lex_tale_english" ~ "English LexTale",
      parameter == "lex_tale_english_pair" ~ "Arabic LexTale"
    ),
    Score = paste0(mean, " (", sd, ")")
  ) |> 
  select(Parameter, Score) |> 
  apa_gt()
```

We used the same analytical techniques in Experiment 2 as we did for Experiment 1. For accuracy and reaction time analyses, the first trial in each block and "recovery" trials following an error were excluded from the analyses. For reaction time analyses only, reaction times under 150ms, or reaction times over 2500ms or more than three standard deviations above the participant mean were discarded as outliers. Taking these criteria into account, a total of `r report_dropped_trial_numbers_accuracy(data_checks$trial_numbers, "arabic_stroop")`% and `r report_dropped_trial_numbers_rt(data_checks$trial_numbers, "arabic_stroop")`% of trials were excluded from the accuracy and reaction time analyses respectively.

Response times and the accuracies are shown in each language split by Stroop Block and Trial Type in @fig-arabic-plot.

```{r fig-arabic-plot}
#| label: fig-arabic-plot
#| fig-cap: "Response times and proportions of correct responses by Language, Stroop Block, and Trial Type in Experiment 2"
#| fig-alt: "A raincloud plot showing the reaction times and proportion of correct responses in English and Arabic."
knitr::include_graphics(here("03_plots", "03_descriptive", "arabic_patchwork.png"))
```

### Response Time

Fixed effects parameter estimates for accuracy models are shown in @tbl-param-arabic-rt.

```{r tbl-param-arabic-rt}
#| label: tbl-param-arabic-rt
#| tbl-cap: Fixed effects parameter estimates and test statistics for response time in Experiment 2
bind_parameters_rt(
  freq_results$rt_mixed_freq_table_est,
  bayes_results$`rt_bayes-factors`,
  "Arabic"
) |> 
  parameters_rt_gt()
```

```{r arabic-rt-emm}
#| include: false
arabic_stroop_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "arabic", analysis == "stroop")

arabic_language_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "arabic", analysis == "language")

arabic_trial_type_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "arabic", analysis == "trial-type")

arabic_language_trial_type_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "arabic", analysis == "trial-type-groupedby-language") |> 
  select(-c(study, response, analysis))

arabic_language_trial_type_rt_pairs <- pairwise_tests$rt_pairs_resp |> 
  filter(study == "arabic", analysis == "trial-type-groupedby-language") |> 
  select(-c(study, response, analysis))

arabic_language_trial_type_rt_pairs_diffs <- pairwise_tests$rt_pairs_diffs_resp |> 
  filter(study == "arabic", analysis == "trial-type-groupedby-language")
```

There was a significant main effect of Stroop Block with faster response times in the Neutral condition `r make_descriptives_sentence(arabic_stroop_rt_emm, "Neutral", "emmean", "lower.CL", "upper.CL")` than the Incongruent condition `r make_descriptives_sentence(arabic_stroop_rt_emm, "Incongruent", "emmean", "lower.CL", "upper.CL")`. There was also a significant main effect of Language, with faster response times in English `r make_descriptives_sentence(arabic_language_rt_emm, "English", "emmean", "lower.CL", "upper.CL")` than Arabic `r make_descriptives_sentence(arabic_language_rt_emm, "Arabic", "emmean", "lower.CL", "upper.CL")`. There was also a significant main effect of Trial Type, with faster response times in the Repetition `r make_descriptives_sentence(arabic_trial_type_rt_emm, "Repetition", "emmean", "lower.CL", "upper.CL")` than the Switch trials `r make_descriptives_sentence(arabic_trial_type_rt_emm, "Switch", "emmean", "lower.CL", "upper.CL")`. Finally, there was a significant interaction between Language and Trial Type. Estimated marginal means for Trial Type within each Language are shown in @tbl-emmeans-arabic-language-trial-type-rt

```{r tbl-emmeans-arabic-language-trial-type-rt}
#| label: tbl-emmeans-arabic-language-trial-type-rt
#| tbl-cap: Estimated marginal means of Response Time for Trial Type by Language Variety in Experiment 2
arabic_language_trial_type_rt_emm |> 
  mutate(`95% CI` = paste0("[", lower.CL, ", ", upper.CL, "]")) |> 
  select(-c(lower.CL, upper.CL)) |> 
  apa_gt() |> 
  cols_label(
    condition = md("Condition"),
    emmean = md("Mean"),
    SE = md("*SE*")
  ) 
```

Pairwise tests show that when the switch cost is present in Arabic `r make_pairwise_sentence(arabic_language_trial_type_rt_pairs |> filter(language == "Arabic"), "Switch / Repetition", "ratio")` and English `r make_pairwise_sentence(arabic_language_trial_type_rt_pairs |> filter(language == "English"), "Switch / Repetition", "ratio")`. However, the magnitude of effects was stronger in the English than Arabic trials `r make_pairwise_sentence(arabic_language_trial_type_rt_pairs_diffs, "(Switch / Repetition Arabic) / (Switch / Repetition English)", "ratio")`. All other effects were non-significant, with Bayes factors in support of the null hypothesis. 

The ANOVA showed a main effect of Stroop Block `r make_anova_sentence(freq_results$rt_anova_freq_table, "Arabic", "Stroop")` and Language `r make_anova_sentence(freq_results$rt_anova_freq_table, "Arabic", "Language")`, but no significant interaction between Stroop Block and Language (*p* > .05)

### Accuracy

Fixed effects parameter estimates for accuracy models are shown in @tbl-param-arabic-accuracy.

```{r tbl-param-arabic-accuracy}
#| label: tbl-param-arabic-accuracy
#| tbl-cap: Fixed effects parameter estimates and test statistics for accuracy in Experiment 2
bind_parameters_accuracy(
  freq_results$accuracy_mixed_freq_table_est,
  bayes_results$`accuracy_bayes-factors`,
  "Arabic"
) |> 
  parameters_accuracy_gt()
```

```{r arabic-accuracy-emm}
#| include: false
arabic_stroop_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "arabic", analysis == "stroop")

arabic_trial_type_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "arabic", analysis == "trial-type")

arabic_language_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "arabic", analysis == "language")

arabic_three_way_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "arabic", analysis == "stroop-by-language-by-trial-type")

arabic_three_way_pairs <- pairwise_tests$accuracy_pairs_resp |> 
  filter(study == "arabic", analysis == "trial-type-groupedby-stroop-by-language")

arabic_three_way_pairs_diffs <- pairwise_tests$accuracy_pairs_diffs_resp |> 
    filter(study == "arabic", analysis == "trial-type-groupedby-stroop-by-language")
```

There was a significant main effect of Stroop Block with a small but significantly larger proportion of correct responses in the Neutral condition `r make_descriptives_sentence(arabic_stroop_accuracy_emm, "Neutral", "prob", "asymp.LCL", "asymp.UCL")` than the Incongruent condition `r make_descriptives_sentence(arabic_stroop_accuracy_emm, "Incongruent", "prob", "asymp.LCL", "asymp.UCL")`. There was also a significant main effect of Language, again with a small but significantly larger proportion of correct responses in English `r make_descriptives_sentence(arabic_language_accuracy_emm, "English", "prob", "asymp.LCL", "asymp.UCL")` than Arabic `r make_descriptives_sentence(arabic_language_accuracy_emm, "Arabic", "prob", "asymp.LCL", "asymp.UCL")`. There was also a significant main effect of Trial Type, with a larger proportion of correct responses for Repetition `r make_descriptives_sentence(arabic_trial_type_accuracy_emm, "Repetition", "prob", "asymp.LCL", "asymp.UCL")` than Switch trials `r make_descriptives_sentence(arabic_trial_type_accuracy_emm, "Switch", "prob", "asymp.LCL", "asymp.UCL")`. While all two-way interactions were non-significant and with Bayes factors in support of the null hypothesis, the Bayes factor was only reliable for the Language × Trial Type interaction. Finally, there was a significant 3-way interaction between Stroop Block, Language, and Trial Type. The pattern of results for this interaction is shown in @fig-arabic-plot.

Pairwise tests show that in Arabic the switch cost is present for Neutral `r make_pairwise_sentence(arabic_three_way_pairs |> filter(language == "Arabic", stroop == "Neutral"), "Switch / Repetition", "odds.ratio")` and Incongruent `r make_pairwise_sentence(arabic_three_way_pairs |> filter(language == "Arabic", stroop == "Incongruent"), "Switch / Repetition", "odds.ratio")` Stroop blocks. However, in English this effect is only present in Neutral `r make_pairwise_sentence(arabic_three_way_pairs |> filter(language == "English", stroop == "Neutral"), "Switch / Repetition", "odds.ratio")` but not Incongruent `r make_pairwise_sentence(arabic_three_way_pairs |> filter(language == "English", stroop == "Incongruent"), "Switch / Repetition", "odds.ratio")` Stroop blocks.

The ANOVA showed a main effect of Language `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Arabic", "Language")`, but no significant main effect of Stroop Block or interaction between Stroop Block and Language (both *p*s > .05).

# Experiment 3

Focusing primarily on the findings related to our primary hypotheses, Experiment 1 (Dutch-English bilinguals) showed no reliable evidence for a three-way interaction between Language, Stroop Block, and Trial Type or for switch costs by Language and Stroop block in aggregated analyses most similar to that used by @liu2019symmetries. Experiment 2 (Arabic-English bilinguals) found evidence for a three-way interaction in accuracy analyses whereby switch costs were only present in Arabic (L1), with no effect in English (L2). However, there was evidence against a two-way interaction of Stroop and Trial Type and for the three-way interaction for response time. Again, ANOVAs most similar to the analyses used by @liu2019symmetries for response times and accuracy found no interaction between Stroop Block and Language.

In Experiment 3, we set out to conduct a replication of Experiment 1 of Liu et al. (2019). To this end, we used the same methodology as the original experiment and relied on the same type of bilinguals (Chinese-English). 

## Participants

`r report_participant_numbers(data_checks$participant_numbers, "chinese_stroop")` Chinese-English bilinguals were recruited. `r report_recording_failure(performance_exclusions, "chinese_stroop")` participant was excluded due to a recording failure, and `r report_performance_exclusions(performance_exclusions, "chinese_stroop")` participants were excluded due to non-compliance with instructions, leaving `r report_participant_counts(descriptives$gender_counts, "chinese_stroop")` participants with a mean age of `r report_participant_age(descriptives$demo_summary, "chinese_stroop")`. Following the experiment, the participants were asked to complete a language background questionnaire [cf. @kirk2018can], and English and Chinese vocabulary tests based on lexical decision tasks [@chan2018lextale_ch; @lemhofer2012introducing\; see @tbl-chinese-demo].

```{r tbl-chinese-demo}
#| label: tbl-chinese-demo
#| tbl-cap: Mean percentages (SD) for language use and proficiency for each language for participants in Experiment 3
descriptives$demo_summary |> 
  filter(
    study == "chinese_stroop", 
    str_detect(parameter, "current_percent|lex_tale"), 
    statistic %in% c("mean", "sd")
  ) |> 
  pivot_wider(names_from = statistic, values_from = value) |> 
  mutate_if(is.numeric, round_pad) |> 
  mutate(
    Parameter = case_when(
      parameter == "current_percent_english" ~ "English Current Use",
      parameter == "current_percent_english_pair" ~ "Chinese Current Use",
      parameter == "lex_tale_english" ~ "English LexTale",
      parameter == "lex_tale_english_pair" ~ "Chinese LexTale"
    ),
    Score = paste0(mean, " (", sd, ")")
  ) |> 
  select(Parameter, Score) |> 
  apa_gt()
```

We used the same analytical techniques in Experiment 3 as we did for Experiments 1 and 2. For accuracy and reaction time analyses, the first trial in each block and "recovery" trials following an error were excluded from the analyses. For reaction time analyses only, reaction times under 150ms, or reaction times over 2500ms or more than three standard deviations above the participant mean were discarded as outliers. Taking these criteria into account, a total of `r report_dropped_trial_numbers_accuracy(data_checks$trial_numbers, "chinese_stroop")`% and `r report_dropped_trial_numbers_rt(data_checks$trial_numbers, "chinese_stroop")`% of trials were excluded from the accuracy and reaction time analyses respectively.

Response times and the proportion of correct responses are shown in each language split by Stroop Block and Trial Type in @fig-chinese-plot.

```{r fig-chinese-plot}
#| label: fig-chinese-plot
#| fig-cap: "Response times and proportions of correct responses by Language, Stroop Block, and Trial Type in Experiment 3"
#| fig-alt: "A raincloud plot showing the reaction times and proportion of correct responses in English and Chinese."
knitr::include_graphics(here("03_plots", "03_descriptive", "chinese_patchwork.png"))
```

### Response Time

Fixed effects parameter estimates for accuracy models are shown in @tbl-param-chinese-rt.

```{r tbl-param-chinese-rt}
#| label: tbl-param-chinese-rt
#| tbl-cap: Fixed effects parameter estimates and test statistics for response time in Experiment 3
bind_parameters_rt(
  freq_results$rt_mixed_freq_table_est,
  bayes_results$`rt_bayes-factors`,
  "Chinese"
) |> 
  parameters_rt_gt()
```

```{r chinese-rt-emm}
#| include: false
chinese_stroop_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "chinese", analysis == "stroop")

chinese_language_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "chinese", analysis == "language")

chinese_trial_type_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "chinese", analysis == "trial-type")

chinese_language_trial_type_rt_emm <- pairwise_tests$rt_emms_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language") |> 
  select(-c(study, response, analysis))

chinese_language_trial_type_rt_pairs <- pairwise_tests$rt_pairs_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language") |> 
  select(-c(study, response, analysis))

chinese_language_trial_type_rt_pairs_diffs <- pairwise_tests$rt_pairs_diffs_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language")
```

There was a significant main effect of Stroop Block with faster response times in the Neutral condition `r make_descriptives_sentence(chinese_stroop_rt_emm, "Neutral", "emmean", "lower.CL", "upper.CL")` than the Incongruent condition `r make_descriptives_sentence(chinese_stroop_rt_emm, "Incongruent", "emmean", "lower.CL", "upper.CL")`. There was also a significant main effect of Language, with faster response times in Chinese `r make_descriptives_sentence(chinese_language_rt_emm, "Chinese", "emmean", "lower.CL", "upper.CL")` than English `r make_descriptives_sentence(chinese_language_rt_emm, "English", "emmean", "lower.CL", "upper.CL")`. There was also a significant main effect of Trial Type, with faster response times in the Repetition `r make_descriptives_sentence(chinese_trial_type_rt_emm, "Repetition", "emmean", "lower.CL", "upper.CL")` than the Switch trials `r make_descriptives_sentence(chinese_trial_type_rt_emm, "Switch", "emmean", "lower.CL", "upper.CL")`. Finally, there was a significant two-way interaction between Language and Trial Type.

Estimated marginal means for Trial Type within each Language are shown in @tbl-emmeans-chinese-language-trial-type-rt

```{r tbl-emmeans-chinese-language-trial-type-rt}
#| label: tbl-emmeans-chinese-language-trial-type-rt
#| tbl-cap: Estimated marginal means of Response Times for Trial Type by Language Variety in Experiment 3
chinese_language_trial_type_rt_emm |> 
  mutate(`95% CI` = paste0("[", lower.CL, ", ", upper.CL, "]")) |> 
  select(-c(lower.CL, upper.CL)) |> 
  apa_gt() |> 
  cols_label(
    condition = md("Condition"),
    emmean = md("Mean"),
    SE = md("*SE*")
  ) 
```

Pairwise tests show that when the switch cost is present in Chinese `r make_pairwise_sentence(chinese_language_trial_type_rt_pairs |> filter(language == "Chinese"), "Switch / Repetition", "ratio")` and English `r make_pairwise_sentence(chinese_language_trial_type_rt_pairs |> filter(language == "English"), "Switch / Repetition", "ratio")`. However, the magnitude of effects was stronger in the Chinese than English trials `r make_pairwise_sentence(chinese_language_trial_type_rt_pairs_diffs, "(Switch / Repetition Chinese) / (Switch / Repetition English)", "ratio")`. All other effects were non-significant, with Bayes factors in support of the null hypothesis, apart from the Stroop × Language two-way interaction where the Bayes factor indicates insufficient evidence in support of either model.

The ANOVA showed a significant main effect of Stroop Block `r make_anova_sentence(freq_results$rt_anova_freq_table, "Chinese", "Stroop")`, Language `r make_anova_sentence(freq_results$rt_anova_freq_table, "Chinese", "Language")`, and a weak but significant interaction between Stroop Block and Language `r make_anova_sentence(freq_results$rt_anova_freq_table, "Chinese", "Stroop × Language")`.

### Accuracy

Fixed effects parameter estimates for accuracy models are shown in @tbl-param-chinese-accuracy.

```{r tbl-param-chinese-accuracy}
#| label: tbl-param-chinese-accuracy
#| tbl-cap: Fixed effects parameter estimates and test statistics for accuracy in Experiment 3
bind_parameters_accuracy(
  freq_results$accuracy_mixed_freq_table_est,
  bayes_results$`accuracy_bayes-factors`,
  "Chinese"
) |> 
  parameters_accuracy_gt()
```

```{r chinese-accuracy-emm}
#| include: false
chinese_stroop_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "chinese", analysis == "stroop")

chinese_trial_type_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "chinese", analysis == "trial-type")

chinese_language_trial_type_accuracy_emm <- pairwise_tests$accuracy_emms_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language")

chinese_language_trial_type_accuracy_pairs <- pairwise_tests$accuracy_pairs_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language")

chinese_language_trial_type_accuracy_pairs_diffs <- pairwise_tests$accuracy_pairs_diffs_resp |> 
  filter(study == "chinese", analysis == "trial-type-groupedby-language")
```

There was a significant main effect of Stroop Block with a larger proportion of correct responses in the Neutral condition `r make_descriptives_sentence(chinese_stroop_accuracy_emm, "Neutral", "prob", "asymp.LCL", "asymp.UCL")` than the Incongruent condition `r make_descriptives_sentence(chinese_stroop_accuracy_emm, "Incongruent", "prob", "asymp.LCL", "asymp.UCL")`. There was also a significant main effect of Trial Type, with a larger proportion of correct responses in the Repetition `r make_descriptives_sentence(chinese_trial_type_accuracy_emm, "Repetition", "prob", "asymp.LCL", "asymp.UCL")` than the Switch trials `r make_descriptives_sentence(chinese_trial_type_accuracy_emm, "Switch", "prob", "asymp.LCL", "asymp.UCL")`. Finally, there was also a significant 2-way interaction between Language and Trial Type.

Estimated marginal means for Trial Type within each Language are shown in @tbl-emmeans-chinese-language-trial-type-accuracy

```{r tbl-emmeans-chinese-language-trial-type-accuracy}
#| label: tbl-emmeans-chinese-language-trial-type-accuracy
#| tbl-cap: Estimated marginal means of Accuracy for Trial Type by Language Variety in Experiment 3
chinese_language_trial_type_accuracy_emm |> 
  mutate(`95% CI` = paste0("[", asymp.LCL, ", ", asymp.UCL, "]")) |> 
  select(-c(study, response, analysis, df, asymp.LCL, asymp.UCL)) |> 
  apa_gt() |> 
  cols_label(
    condition = md("Condition"),
    prob = md("Mean"),
    SE = md("*SE*")
  ) 
```

Pairwise tests show that when the switch cost is present in Chinese `r make_pairwise_sentence(chinese_language_trial_type_accuracy_pairs |> filter(language == "Chinese"), "Switch / Repetition", "odds.ratio")` and English `r make_pairwise_sentence(chinese_language_trial_type_accuracy_pairs |> filter(language == "English"), "Switch / Repetition", "odds.ratio")`. However, the magnitude of effects was stronger in the Chinese than English trials `r make_pairwise_sentence(chinese_language_trial_type_accuracy_pairs_diffs, "(Switch / Repetition Chinese) / (Switch / Repetition English)", "odds.ratio")`. All other effects were non-significant. Here, Bayes factors were only reliably in support of the null hypothesis for the two-way interaction of Stroop × Trial Type and the three-way interaction of Stroop × Language × Trial Type. The main effect of Language had insufficient evidence in favour of either model, while the two-way interaction of Stroop × Language, while in support of the null hypothesis, was sensitive to the prior specification and thus must be interpreted with caution.

The ANOVA showed a significant main effect of Stroop Block `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Chinese", "Stroop")`, Language `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Chinese", "Language")`, and a weak but significant interaction between Stroop Block and Language `r make_anova_sentence(freq_results$accuracy_anova_freq_table, "Chinese", "Stroop × Language")`.

# Summary of Results

Focusing primarily on the findings related to our primary hypotheses, that of a switch cost dependent upon Language and Stroop Block, Experiment 1 (Dutch-English bilinguals) found evidence against a three-way interaction in response time and accuracy analyses. For the response time analyses only, there was also evidence against a two-way interaction between Stroop Block and Trial Type. While this was non-significant in the accuracy analyses the Bayes factor did not meet the threshold for reliable evidence against the effect. ANOVAs most similar to analyses used by @liu2019symmetries for switch costs to response times and accuracies showed no interaction between Stroop Block and Language. Experiment 2 (Arabic-English bilinguals) found evidence against a two-way interaction between Stroop and Trial Type in response time and accuracy analyses. For response times, there was also evidence against a three-way interaction. However, accuracy analyses showed a significant three-way interaction. ANOVAs for response times and accuracy again found no interaction between Stroop Block and Language. Experiment 3 (Chinese-English bilinguals) showed evidence against a two-interaction of Stroop Block and Trial Type and against a three-way interaction in response time and accuracy analyses. However, here ANOVAs found weak but significant interactions between Stroop Block and Language.

Overall, switch costs reliant upon Language and Stroop Block were only found in Arabic-English bilinguals for accuracy analyses and in Chinese-English bilinguals for response times and accuracy when data are aggregated by participants. Overall, this indicates no reliable effect for switch costs dependent upon Language and Stroop Block in Dutch-English, Arabic-English, and Chinese-English bilinguals. 

# References

::: {#refs}
:::

# Appendices

## Appendix A. Frequentist Model Specification

Where models did not converge we took the strategy of removing correlations between random effects before removing first three-way, then two-way interactions, before removing main effects. This criteria was applied to random effects for items before participants under the assumption that, with carefully developed items, variance will be greater in participants than items such that estimates for these random effects will be larger. All models used the bobyqa optimiser.

Following this criteria, the final R formulae were used:

### Experiment 1

#### Reaction Time

    log_rt ~ stroop * language * trial_type + 
      (1 + stroop + language + trial_type + stroop:language + trial_type:language | 
      subject_id) + 
      (1 + trial_type | word_colour)

#### Accuracy

    correct ~ stroop * trial_type * language +
      (1 + stroop + trial_type + language || subject_id) + 
      (1 | word_colour)

### Experiment 2

#### Reaction Time

    log_rt ~ stroop * language * trial_type + 
      (1 + stroop + language + trial_type + stroop:language + trial_type:language || 
      subject_id) + 
      (1 + trial_type | word_colour)

#### Accuracy

    correct ~ stroop * trial_type * language +
      (1 | subject_id)

### Experiment 3

#### Reaction Time

    log_rt ~ stroop * language * trial_type + 
      (1 + stroop + language + trial_type + stroop:language + trial_type:language || 
      subject_id) + 
      (1 + trial_type | word_colour)

#### Accuracy

    correct ~ stroop * trial_type * language +
      (1 + stroop + language | subject_id) + 
      (1 + trial_type | word_colour)

## Appendix B. Bayesian Model Specification

### Model Priors

All models used the maximal random effects structure. For each experiment five models were fitted for response time and accuracy models respectively with priors varying across models for the precision of main effects and interactions for the fixed effects.

#### Reaction Time

We began with setting priors for a single model in each Experiment with priors based on findings from [@liu2019symmetries]. The grand mean in Experiment 1 of Liu et al. is 1036.75ms (approx 6.9 on log scale). Thus, for our intercept we used a prior with a range of 800 - 1200ms, $Normal(6.9, 0.1)$. Liu et al. found strong effects of Language (i.e. with a $\eta_{p}^{2}$ of 0.4). The biggest difference between languages was around 100ms. Thus, for language we used a prior with a range of -220ms to 220ms, $Normal(0, 0.05)$. While Liu found effects of Stroop half as big as those for language, we assume that an effect at least as big as that of Language should be found for the effect Stroop blocks, so the same prior is used here. Trial type was not calculated in Liu et al., so we again assume the same prior for this main effect. All interactions as in Liu are at least half the size of the main effects, so we make the same assumption in our priors, setting all interactions to have priors of $Normal(0, 0.025)$. 

We then varied priors around this baseline to evaluate their impact on fixed effect parameter estimates and Bayes factors. We varied main effects with *SD*s from 0.01, 0.025, 0.05, 0.075, 0.1, and with interactions with *SD*s from 0.005, 0.01, 0.025, 0.05, 0.075. We chose the final model for reporting where parameter estimates and Bayes factors were relatively stable. This is model 4 for each experiment and outcome. Finally, all models had the same priors for the remaining terms. We used a $Normal(0, 0.1)$ prior for all group-level (random) effects with an $LKJ(2)$ prior for the correlations between group-level effects, which regularises correlations to put less weight on correlations of 0 and 1. Finally, we used a $Normal(0, 0.15)$ prior for the sigma term.

#### Accuracy

Again, we began with setting priors for a single model in each Experiment with priors based on findings from [@liu2019symmetries]. In Experiment 1 of Liu et al. (2019) there is a very high grand mean accuracy of .93 (a logit of approximately 2.6). We thus use a prior with an approximate range of 2.2–3 logits, $Normal(2.6, 0.2)$. Liu et al. found relatively strong Stroop effects of around 2% across conditions. This equates to the difference between an inverse logit of .93 and .91, i.e. a logit of 0.27. We thus set all main effects to allow for effects twice as large as that found in Liu et al. either side of 0, i.e. with a prior of $Normal(0, 0.27)$. Given scores are assumed to be close to ceiling we allowed for broader priors on interaction effects when compared to the reaction time models, allowing for interaction effects around 1/3 as large as main effects, $Normal(0., 0.18)$. 

We again varied priors around this baseline to evaluate their impact on fixed effect parameter estimates and Bayes factors. We varied main effects with *SD*s from 0.09, 0.18, 0.27, 0.36, 0.45, and with interactions with *SD*s from 0.045, 0.09, 0.18, 0.27, 0.36. We chose the final model for reporting where parameter estimates and Bayes factors were relatively stable. This is model 4 for each experiment and outcome. Finally, all models had the same priors for the remaining terms. We used a $Normal(0, 0.5)$ prior for all group-level (random) effects, exluding the intercept for by-participant effects which had a broader prior of $Normal(0, 2)$. We also used an $LKJ(2)$ prior for the correlations between group-level effects, which regularises correlations to put less weight on correlations of 0 and 1.

### Sensitivity of Parameter Estimates

The following plot shows how sensitive parameter estimates are to the prior specification. Note that only priors that differ across models are highlighted in the plots below.

#### Reaction Time

```{r fig-prior-fixef-rt}
#| label: fig-prior-fixef-rt
#| fig-cap: "Sensitivity of fixed effect parameter estimates to prior specification in the Bayesian models for reaction time across studies."
#| fig-alt: "Plots showing how fixed effect parameter estimates change across Bayesian models with different prior specifications for the reaction time outcome. Parameter estimates are typically stable between the 3rd and 5th models."
knitr::include_graphics(here("03_plots", "02_sensitivity-checks", "02_bayesian", "rt_fixef.png"))
```

#### Accuracy

```{r fig-prior-fixef-accuracy}
#| label: fig-prior-fixef-accuracy
#| fig-cap: "Sensitivity of fixed effect parameter estimates to prior specification in the Bayesian models for accuracy across studies."
#| fig-alt: "Plots showing how fixed effect parameter estimates change across Bayesian models with different prior specifications for the accuracy outcome. Parameter estimates are typically stable between the 3rd and 5th models."
knitr::include_graphics(here("03_plots", "02_sensitivity-checks", "02_bayesian", "accuracy_fixef.png"))
```

### Sensitivity of Bayes Factors

Similarly, we show how prior specification affects the direction and magnitude of Bayes factors across models and studies. Here , the grey band shows the region where insuffient evidence is found for the null and alternative hypotheses. Values above this band show evidence in support of the null hypothesis. Values below this line show evidence in support of the alternative hypothesis.

#### Reaction Time

```{r fig-prior-bf-rt}
#| label: fig-prior-bf-rt
#| fig-cap: "Sensitivity of Bayes factors to prior specification in the Bayesian models for response times across studies."
#| fig-alt: "Plots showing how Bayes factors change across Bayesian models with different prior specifications for the response time outcome. Conclusions are often consistent between the 3rd and 5th models."
knitr::include_graphics(here("03_plots", "02_sensitivity-checks", "02_bayesian", "rt_bayes-factors.png"))
```

#### Accuracy

```{r fig-prior-bf-accuracy}
#| label: fig-prior-bf-accuracy
#| fig-cap: "Sensitivity of Bayes factors to prior specification in the Bayesian models for accuracy across studies."
#| fig-alt: "Plots showing how Bayes factors change across Bayesian models with different prior specifications for the accuracy outcome. Conclusions are often consistent between the 3rd and 5th models."
knitr::include_graphics(here("03_plots", "02_sensitivity-checks", "02_bayesian", "accuracy_bayes-factors.png"))
```
